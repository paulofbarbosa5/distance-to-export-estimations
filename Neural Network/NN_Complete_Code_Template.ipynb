{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79770371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD\n",
    "# Rever função CPI está bem feita\n",
    "\n",
    "#References\n",
    "\n",
    "#Feature Selection\n",
    "#https://www.shiksha.com/online-courses/articles/feature-selection-techniques-python-code/\n",
    "#https://www.datatechnotes.com/2022/10/feature-selection-example-with-rfecv-in.html\n",
    "#https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "#https://medium.com/@Kavya2099/optimizing-performance-selectkbest-for-efficient-feature-selection-in-machine-learning-3b635905ed48\n",
    "#https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "#https://www.quora.com/Why-does-the-model-have-high-accuracy-on-test-data-but-lower-with-cross-validation-in-machine-learning\n",
    "#https://www.quora.com/Can-validation-accuracy-be-higher-than-training-accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96125359",
   "metadata": {},
   "source": [
    "Variaveis referencia \n",
    "\n",
    "exporterIntensity ~ FEATURES",
    "tirar estas excepto periodo anterior\n",
    "-c(NIPC, Ano, exporterIntensity, ExporIES_euros, ExporINE_euros)\n",
    "\n",
    "sendo que exporterIntensity é a variável outcome\n",
    "\n",
    "div10 setor a 2 digitos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df23ef5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Imports ##################################################################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import shap\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from numpy import array  \n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nipcs=pd.read_csv(\"NIPCs_test.csv\")\n",
    "file = pd.read_csv('C:/Users/jcortes/Aicep Portugal Global/Paulo Barbosa - paper_ML/Data/data5.csv',encoding='unicode_escape')  \n",
    "infla = pd.read_excel(\"CPI_2010-2021.xlsx\")\n",
    "destinations = pd.read_csv(\"firms_destinations.csv\")\n",
    "\n",
    "def cpi_deflated(file, infla, nominal_variables): \n",
    "    \"\"\"Adapts the code for all years and columns.\n",
    "    Args:\n",
    "        file (pd.DataFrame): Input DataFrame.\n",
    "        infla (pd.DataFrame): DataFrame containing inflation data.\n",
    "        nominal_variables (list): List of column names to adjust.\n",
    "    Returns:\n",
    "        pd.DataFrame: Modified DataFrame with adjusted columns.\n",
    "    \"\"\"\n",
    "    for col in nominal_variables:\n",
    "        for year in file['Ano'].unique():\n",
    "            if year not in infla['Ano'].values:\n",
    "                print(f\"Warning: Year {year} missing in infla. Skipping column {col}.\")\n",
    "                continue\n",
    "            indice = infla[infla['Ano'] == year]['indice2010'].values[0]\n",
    "            file.loc[file['Ano'] == year, col] = np.where(\n",
    "                file.loc[file['Ano'] == year, col], file.loc[file['Ano'] == year, col] / indice, 0)\n",
    "    return file\n",
    "\n",
    "# Data Curation Functions\n",
    "def Exporter_cat(flx):\n",
    "    NIPCs_Const_NonExport=[] #Constant non-exporters\n",
    "    NIPCs_Const_Export=[] #Constant exporters\n",
    "    NIPCs_Swit_Export=[] #Switching Exporters 0 t1 1 1\n",
    "    NIPCs_Swit_NonExport=[] #Switching Non-Exporters 1 t0 0 \n",
    "    NIPCs_Disc_Export=[] #Discontinuos exporters 0 1 0 ou 1 0 1\n",
    "    Non_Exp=[]\n",
    "    for nipc in list(set(flx.NIPC)):\n",
    "        data = list(flx[flx['NIPC']==nipc].sort_values(by=\"Ano\").Exporter.diff())    \n",
    "        positive_sum = sum(x for x in data if x > 0)\n",
    "        negative_sum = sum(x for x in data if x < 0)\n",
    "        if len(set(flx[flx['NIPC']==nipc].Exporter))== 1 and list(set(flx[flx['NIPC']==nipc].Exporter))[0] == 0 :\n",
    "            NIPCs_Const_NonExport.append(nipc)\n",
    "        elif len(set(flx[flx['NIPC']==nipc].Exporter))== 1 and list(set(flx[flx['NIPC']==nipc].Exporter))[0] == 1:\n",
    "            NIPCs_Const_Export.append(nipc)\n",
    "        elif len(set(flx[flx['NIPC']==nipc].Exporter))== 2 and (positive_sum > 1 or negative_sum <-1) or (positive_sum == 1 and negative_sum == -1) :\n",
    "            NIPCs_Disc_Export.append(nipc)\n",
    "        elif len(set(flx[flx['NIPC']==nipc].Exporter))== 2 and positive_sum == 1 and negative_sum == 0:\n",
    "            NIPCs_Swit_Export.append(nipc) #\n",
    "        elif len(set(flx[flx['NIPC']==nipc].Exporter))== 2 and negative_sum == -1 and positive_sum == 0:\n",
    "            NIPCs_Swit_NonExport.append(nipc) \n",
    "        else:\n",
    "            Non_Exp.append(nipc)\n",
    "    return NIPCs_Const_NonExport, NIPCs_Const_Export, NIPCs_Swit_Export, NIPCs_Swit_NonExport,NIPCs_Disc_Export, Non_Exp\n",
    "\n",
    "def count_consecutive_ones(series):\n",
    "        return series.rolling(window=4, min_periods=1).sum()\n",
    "\n",
    "def count_consecutive_twos(series):\n",
    "        return series.rolling(window=2, min_periods=1).sum()\n",
    "\n",
    "def preparing_train_test(tflx):\n",
    "\n",
    "    # split into X and Y\n",
    "    Y = tflx[['Exporter']].reset_index()\n",
    "    X = tflx.drop(['Exporter'], axis=1)\n",
    "\n",
    "    cols = X.columns  \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    X=pd.DataFrame(X, columns = cols, index = tflx.index).reset_index()\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "\n",
    "    #Original\n",
    "    X_test = X[X.NIPC_Ano.str[:9].str.contains('|'.join(list(set(nipcs.value.astype(str)))))] \n",
    "    X_train = X[~(X.NIPC_Ano.str[:9].str.contains('|'.join(list(set(nipcs.value.astype(str))))))] \n",
    "    y_test = Y[Y.NIPC_Ano.str[:9].str.contains('|'.join(list(set(nipcs.value.astype(str)))))] \n",
    "    y_train = Y[~(Y.NIPC_Ano.str[:9].str.contains('|'.join(list(set(nipcs.value.astype(str))))))] \n",
    "\n",
    "    #training will all but year selected for test\n",
    "    #X_test = X[X.NIPC_Ano.str[9:]==\"2012\"] \n",
    "    #X_train = X[~(X.NIPC_Ano.str[9:]==\"2012\")] \n",
    "    #y_test = Y[Y.NIPC_Ano.str[9:]==\"2012\"] \n",
    "    #y_train = Y[~(Y.NIPC_Ano.str[9:]==\"2012\")] \n",
    "\n",
    "    #Alternative train/test split\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=7777)   \n",
    "        \n",
    "    X_train = X_train.set_index(\"NIPC_Ano\")#.drop(['NIPC_Ano'], axis=1)\n",
    "    X_test = X_test.set_index(\"NIPC_Ano\")#.drop(['NIPC_Ano'], axis=1)\n",
    "    y_train = y_train.set_index(\"NIPC_Ano\")#.drop(['NIPC_Ano'], axis=1)\n",
    "    y_test = y_test.set_index(\"NIPC_Ano\")#.drop(['NIPC_Ano'], axis=1)\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    return X_test, X_train, y_test, y_train\n",
    "\n",
    "#Feature selection by using Mutual Information Gain\n",
    "#The higher the value the more important that feature will be or you can say that the dependency of that independent \n",
    "#feature will be more on the dependent feature.\n",
    "def Feature_Selection(X_train,y_train, relative_weight , nr_weight):\n",
    "    mutual_info = mutual_info_classif(X_train, y_train)\n",
    "    mutual_info = pd.Series(mutual_info)\n",
    "    mutual_info.index = X_train.columns\n",
    "    mutual_info.sort_values(ascending=False)\n",
    "    #mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8)) #Permitiu ver que as principais 150 têm a maior significancia\n",
    "    mif= pd.DataFrame(mutual_info.sort_values(ascending=False))\n",
    "    if relative_weight == True:\n",
    "        mif[\"Cum\"]=mif[0].cumsum()\n",
    "        mif[\"Relative_Sum\"]=mif[\"Cum\"]/mif[\"Cum\"][-1]\n",
    "        mutual_info_cols = list(mif[mif[\"Relative_Sum\"] < nr_weight].index)\n",
    "        nr_feats = len(mutual_info_cols)\n",
    "    else:\n",
    "        nr_feats = nr_weight\n",
    "        mutual_info_cols = list(mif[:nr_feats].index)\n",
    "    #Select K Best\n",
    "    #No we Will select the top nr_feats important features\n",
    "    sel_five_cols = SelectKBest(mutual_info_classif, k=nr_feats)\n",
    "    sel_five_cols.fit(X_train, y_train)\n",
    "    X_train.columns[sel_five_cols.get_support()]\n",
    "    select_K_rec=list(X_train.columns[sel_five_cols.get_support()])\n",
    "    #Getting top nr_feats\n",
    "    to_incl=list(set(select_K_rec) - set(mutual_info_cols))\n",
    "    #Because data consists of both categorical and numerical variables and want to perform a classification problem, \n",
    "    #“mutual_info_classif” might be a better choice.\n",
    "    #Will use mutual_info_classif top 150 and will add top_incl: K-Best features ignored by mutual info that can be relevant\n",
    "    final_features = list(mutual_info.sort_values(ascending=False)[:nr_feats].index) + to_incl\n",
    "    return final_features\n",
    "\n",
    "#Método adicional RFECV - leva bastante tempo\n",
    "def RFE(X_train,y_train):\n",
    "    x = X_train\n",
    "    y = y_train\n",
    "    rfc = RandomForestClassifier()\n",
    "    select = RFECV(estimator=rfc, cv=10)\n",
    "    select = select.fit(x,y)\n",
    "    print(\"Feature ranking: \", select.ranking_)\n",
    "    mask = select.get_support()\n",
    "    features = array(X_train.columns) \n",
    "    best_features = features[mask]\n",
    "    print(\"All features: \", x.shape[1])\n",
    "    print(features)\n",
    "    print(\"Selected best: \", best_features.shape[0])\n",
    "    print(features[mask]) \n",
    "    return features, mask  \n",
    "\n",
    "#Exploratory Data Analysis / Data Curation / Feature Engineering ##############################################################\n",
    "\n",
    "NIPCs_Micro = [str(a) for a in list(set(file[file.micro==1].NIPC))]\n",
    "NIPCs_Small = [str(a) for a in list(set(file[file.small==1].NIPC))]\n",
    "NIPCs_Medium = [str(a) for a in list(set(file[file.medium==1].NIPC))]\n",
    "NIPCs_Large = [str(a) for a in list(set(file[file.large==1].NIPC))]\n",
    "\n",
    "\n",
    "#1) Predictions with NN for different definitions of exporter - Firms that sell at least 5% of total sales abroad\n",
    "\n",
    "file[\"MercadoExterno_Vendas_Racio\"] = (file[\"Total_Vendas\"] - file[\"MercadoInterno_Vendas\"])/file[\"Total_Vendas\"]\n",
    "\n",
    "file[\"Export_at_least_5_p_cent_abroad\"] = np.where(file[\"MercadoExterno_Vendas_Racio\"]>0.05,1,0 )\n",
    "\n",
    "\n",
    "\n",
    "#Start by removing columns that have too little information\n",
    "filee = file.drop(columns = [\"Expor\", \"N_produtos_expor\", \"Quantidades_expor\", \"N_produtos\",\n",
    "                            \"Valor_medio\", \"ValorMedioExpor\",\"Quantidades\",\"MercadoComunitario_Vendas\",\n",
    "                             \"MercadoExtraComunitario_Vendas\", \"small\", \"micro\", \"medium\",\"large\",\n",
    "                            \"CustoMercadoriasMateriasConsumidas_Materias\",\"CustosPessoal_Total\",\n",
    "                            \"Total_Fornecimentos\",\"CustoMercadoriasMateriasConsumidas_Total\",\n",
    "                            \"custo_total\",\"PessoasAoServicoEmpresaRemuneradasENaoRemuneradas2\",\n",
    "                            \"PessoasAoServicoEmpresaRemuneradasENaoRemuneradas\"       ])\n",
    "\n",
    "#NIPC Ano\n",
    "filee[\"NIPC_Ano\"]= filee[\"NIPC\"].astype(str)+filee[\"Ano\"].astype(str)\n",
    "\n",
    "#Dependent variable\n",
    "filee[\"expIntensity\"] = filee[\"ExporIES_euros\"]/filee[\"Total_Vendas\"]\n",
    "filee['exporter'] = np.where(filee['expIntensity']>0.1, 1, 0)\n",
    "\n",
    "#Deflating nominal values\n",
    "nominal_variables = [\"ProveitosGanhos\", 'ResultadoLiquidoExercicio', 'TotalActivo', 'TotalCapitalProprio',\n",
    "'MercadoInterno_Vendas', 'MercadoInterno_PrestacoesServicos','MercadoInterno_Compras', 'MercadoInterno_Fornecimentos',\n",
    "'MercadoComunitario_PrestacoesServicos','MercadoComunitario_Compras', \n",
    "'MercadoComunitario_Fornecimentos','MercadoExtraComunitario_PrestacaoesServicos',\n",
    "'MercadoExtraComunitario_Compras','MercadoExtraComunitario_Fornecimentos', 'Total_Vendas','Total_PrestacoesServicos', \n",
    "'Total_Compras', 'CustosPessoal_Remuneracoes', \n",
    "'CustoMercadoriasMateriasConsumidas_Mercadorias', 'expIntensity',\n",
    "'CustosPessoal_Outros','ExporIES_euros', 'VolumeNegocios',\n",
    "'VAB','capitalIntensity', 'exportIntensity','exporterIntensity', 'ExporINE_euros']\n",
    "file_d = cpi_deflated(filee.copy(), infla.copy(), nominal_variables) #taken as given from here\n",
    "\n",
    "#Adding dummy year\n",
    "file_d['dummy']=1\n",
    "Years=file_d.pivot(index=\"NIPC_Ano\",columns=\"Ano\", values=\"dummy\").fillna(0)\n",
    "Years = Years.add_prefix('Y_')\n",
    "fil = file_d.merge(Years, on=\"NIPC_Ano\")\n",
    "\n",
    "#Adding dummy division\n",
    "Division= file_d.pivot(index=\"NIPC_Ano\",columns=\"Divisao\", values=\"dummy\").fillna(0)\n",
    "Division = Division.add_prefix('Div_')\n",
    "fil_ = fil.merge(Division, on=\"NIPC_Ano\")\n",
    "\n",
    "#Adding dummy CAE\n",
    "#CAE = file_d.pivot(index=\"NIPC_Ano\",columns=\"CAE\", values=\"dummy\").fillna(0)\n",
    "#CAE = CAE.add_prefix('CAE_')\n",
    "#fil__ = fil_.merge(CAE, on=\"NIPC_Ano\")\n",
    "\n",
    "#Adding dummy Grupo\n",
    "#Grupo = file_d.pivot(index=\"NIPC_Ano\",columns=\"Grupo\", values=\"dummy\").fillna(0)\n",
    "#Grupo = Grupo.add_prefix('Grupo_')\n",
    "#fil___ = fil__.merge(Grupo, on=\"NIPC_Ano\")\n",
    "\n",
    "#Adding dummy Classe\n",
    "#Classe = file_d.pivot(index=\"NIPC_Ano\",columns=\"Classe\", values=\"dummy\").fillna(0)\n",
    "#Classe = Classe.add_prefix('Classe_')\n",
    "#fil____ = fil___.merge(Classe, on=\"NIPC_Ano\")\n",
    "\n",
    "#Adding dummy Dsc_6\n",
    "#Dsc_6 = file_d.pivot(index=\"NIPC_Ano\",columns=\"Dsc_6\", values=\"dummy\").fillna(0)\n",
    "#Dsc_6 = Dsc_6.add_prefix('Dsc_6_')\n",
    "#fil_____ = fil____.merge(Dsc_6, on=\"NIPC_Ano\")\n",
    "\n",
    "#One Hot Encoding for other features if relevant\n",
    "#file_d['Ano'] = file_d['Ano'].astype('category')    \n",
    "# Assigning numerical values and storing it in another columns \n",
    "#data['Gen_new'] = data['Gender'].cat.codes \n",
    "#data['Rem_new'] = data['Remarks'].cat.codes  \n",
    "# Create an instance of One-hot-encoder \n",
    "#enc = OneHotEncoder() \n",
    "# Passing encoded columns \n",
    "#enc_data = pd.DataFrame(enc.fit_transform(file_d[['Ano']]).toarray()) \n",
    "# Merge with main \n",
    "#New_df = file_d.join(enc_data) \n",
    "#one_hot_encoded_data  = pd.get_dummies(file_d, columns = ['Year']) \n",
    "#print(one_hot_encoded_data)\n",
    "\n",
    "#Creating lagged features: \"expIntensity\",\"ExporIES_euros\",\"Total_Vendas\"\n",
    "#dup_nipc=fil_[fil_.NIPC.duplicated(keep=False)]\n",
    "#ndup_nipc=fil_[~fil_.NIPC.duplicated(keep=False)]\n",
    "#dup_nipc.loc[:, 'lag_expIntensity'] = dup_nipc.sort_values('Ano').groupby('NIPC')['expIntensity'].shift(1)\n",
    "#dup_nipc.loc[:, 'lag_ExporIES_euros'] = dup_nipc.sort_values('Ano').groupby('NIPC')['ExporIES_euros'].shift(1)\n",
    "#dup_nipc.loc[:, 'lag_Total_Vendas'] = dup_nipc.sort_values('Ano').groupby('NIPC')['Total_Vendas'].shift(1)\n",
    "df = fil_ #pd.concat([dup_nipc,ndup_nipc])\n",
    "\n",
    "#After vif, multicolinearity and RFE, decided to remove these variables\n",
    "\n",
    "columns_to_drop = [\"dummy\",\"Unnamed: 0\",\"NIPC\",\"Ano\",\"CAE\",\"ExporIES_euros\",\"Seccao\",\"Seccao_dsc\",\"Divisao\",\"Divisao_dsc\",\n",
    "                   \"Grupo\",\"Grupo_dsc\",\"Classe\",\"Classe_dsc\",\"Dsc_6\",\"Seccao_dsc_en\",\"exportIntensity\",\"exporterIntensity\",\n",
    "                   \"ExporINE_euros\",\"expIntensity\",'Total_Vendas','ProveitosGanhos']\n",
    "id_col = \"NIPC_Ano\"\n",
    "\n",
    "exp_variable = \"exporter\"\n",
    "\n",
    "df = df.drop(columns = columns_to_drop)\n",
    "\n",
    "#Removing features with correlation higher than 90%\n",
    "#Exploring correlations in data\n",
    "#df.corr().to_excel(\"correlation_matrix.xlsx\")\n",
    "#ff.to_excel(\"ff.xlsx\")\n",
    "#cor=pd.read_excel(\"correlation_matrix.xlsx\")\n",
    "#cor=cor.set_index(\"Unnamed: 0\")\n",
    "# Remove redundant correlations (using `np.triu` ensures upper triangle)\n",
    "#corr_triu = cor.where(~np.tril(np.ones(cor.shape)).astype(bool))\n",
    "#corfr=pd.DataFrame(corr_triu.stack()).reset_index()\n",
    "#ff=corfr[~(corfr[0]==1)].sort_values(by=[0]).tail(400)\n",
    "#lsx= list(corfr[~(corfr[0]==1)].sort_values(by=[0])[:-400].tail(300)['Unnamed: 0'])\n",
    "#clss=[x for x in lsx if \"Classe\" in x]\n",
    "#caex=[x for x in lsx if \"CAE\" in x]\n",
    "#Classe CAE\n",
    "#ff[ff.apply(lambda x: x[\"Unnamed: 0\"] in x[\"level_1\"], axis=1)]\n",
    "#list(set(caex))+ list(set(clss))\n",
    "#Casos onde CAE e classe tem o mesmo numero\n",
    "# Filter rows where there is a substring in both columns\n",
    "#filtered_df = ff[ff.apply(lambda x: any(y in x[\"Unnamed: 0\"] for y in x[\"level_1\"].split()), axis=1)]\n",
    "\n",
    "#Eventual vif\n",
    "#from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "## the independent variables set \n",
    "#X = df  \n",
    "## VIF dataframe \n",
    "#vif_data = pd.DataFrame() \n",
    "#vif_data[\"feature\"] = X.columns \n",
    "## calculating VIF for each feature \n",
    "#vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))] \n",
    "#vif_data.corr().to_excel(\"vif_data.xlsx\")\n",
    "\n",
    "#C/ 70k obs estes valores pareceram ter uma correlacao demasiado elevada com outros, parecendo a mesma informação\n",
    "#Evidência de multicolinearidade\n",
    "#values_too_high=['CAE_10130.0','CAE_20130.0','Classe_2829','CAE_10414.0','CAE_15120.0','CAE_24410.0','CAE_17211.0','Classe_2540',\n",
    "# 'Grupo_254','CAE_31030.0','Classe_1041','CAE_13941.0','CAE_25401.0','CAE_24420.0','CAE_11071.0','CAE_10420.0','CAE_22112.0',\n",
    "# 'CAE_17230.0','Div_19','CAE_10840.0','CAE_25732.0','CAE_25720.0','CAE_28120.0','CAE_32994.0','CAE_10830.0','CAE_23992.0',\n",
    "# 'CAE_26400.0','CAE_14133.0','CAE_10203.0','CAE_17220.0','CAE_10810.0','CAE_17212.0','Grupo_106','CAE_21201.0','CAE_23413.0',\n",
    "# 'CAE_26512.0','CAE_31010.0','CAE_28250.0','CAE_28150.0','CAE_13991.0','CAE_30111.0','CAE_28940.0','CAE_16220.0','CAE_23132.0',\n",
    "# 'CAE_16213.0','CAE_32400.0','CAE_10510.0','Classe_1102','CAE_32501.0','CAE_16294.0','CAE_10394.0','CAE_16292.0',\n",
    "# 'Classe_1071','CAE_25992.0','CAE_14120.0','CAE_16230.0','Classe_1082','CAE_23414.0','CAE_14140.0','CAE_20141.0','CAE_13961.0',\n",
    "# 'CAE_25734.0','CAE_30112.0','CAE_26600.0','Grupo_267','CAE_28110.0','CAE_11050.0','CAE_10893.0','CAE_25931.0','CAE_28222.0',\n",
    "# 'CAE_10520.0','CAE_24330.0','CAE_22230.0','CAE_14132.0','Classe_2550','CAE_20530.0','CAE_18140.0','CAE_23610.0','CAE_28292.0',\n",
    "# 'CAE_16295.0','Classe_2352','CAE_15202.0','CAE_23321.0','CAE_28920.0','CAE_28991.0','CAE_25933.0','CAE_23140.0','Classe_1413',\n",
    "# 'CAE_10711.0','CAE_18200.0','CAE_28490.0','Classe_1520','CAE_20593.0','CAE_13920.0','CAE_31093.0','CAE_22292.0',\n",
    "# 'CAE_20591.0','Classe_2399','CAE_29320.0','CAE_20510.0','CAE_32122.0','CAE_22192.0','CAE_10860.0','CAE_27110.0','CAE_16291.0',\n",
    "# 'CAE_32300.0','CAE_13302.0','CAE_14310.0','CAE_23620.0','CAE_10712.0','CAE_23690.0','CAE_14190.0','CAE_10611.0','CAE_10120.0',\n",
    "# 'CAE_13992.0','CAE_28930.0','Classe_2331','CAE_10822.0','CAE_10850.0','CAE_23120.0','CAE_25710.0','CAE_10913.0','CAE_14390.0',\n",
    "# 'CAE_20160.0','Classe_2670','Classe_2651','Classe_1061','CAE_13910.0','CAE_10720.0','CAE_23411.0','CAE_25120.0','CAE_28293.0',\n",
    "# 'CAE_26511.0','CAE_23650.0','Classe_3299','CAE_23701.0','CAE_25290.0','CAE_25300.0','CAE_19201.0','Classe_1511','CAE_23640.0',\n",
    "# 'CAE_20520.0','Classe_2211','CAE_11030.0','CAE_25920.0','CAE_10912.0','CAE_10821.0','CAE_29200.0','CAE_30120.0','CAE_27510.0',\n",
    "# 'CAE_30990.0','CAE_13930.0','Classe_3011','CAE_25110.0','CAE_25210.0','CAE_11040.0','Classe_1101','CAE_10110.0','CAE_24430.0',\n",
    "# 'CAE_10392.0','Classe_2899','Classe_3109','CAE_21100.0','CAE_11013.0','CAE_16240.0','Classe_3250','CAE_13201.0','CAE_17120.0',\n",
    "# 'CAE_26120.0','CAE_24200.0','Classe_1107','CAE_23630.0','CAE_20142.0','CAE_16101.0','CAE_11021.0','CAE_14131.0','CAE_13993.0',\n",
    "# 'Classe_2229','CAE_10204.0','CAE_18110.0','CAE_23703.0','CAE_25610.0','CAE_25620.0','CAE_31020.0','CAE_13942.0','CAE_10395.0',\n",
    "# 'CAE_13101.0','CAE_26701.0','CAE_31094.0','CAE_27122.0','CAE_22220.0','CAE_24540.0','Grupo_255','CAE_10393.0','CAE_26110.0',\n",
    "# 'Grupo_329','CAE_27400.0','CAE_23312.0','Classe_2219','CAE_10892.0','CAE_20420.0','CAE_28221.0','CAE_31092.0','CAE_10201.0',\n",
    "# 'CAE_24310.0','CAE_16293.0','Classe_1610','Classe_1089','CAE_18130.0','CAE_15201.0','CAE_28300.0','Classe_1091','Classe_2313',\n",
    "# 'CAE_32200.0','CAE_28210.0','CAE_10310.0','Classe_2120','CAE_25501.0','CAE_23131.0','CAE_18120.0','CAE_21202.0','CAE_20301.0',\n",
    "# 'CAE_30300.0','CAE_20592.0','Classe_2573','Classe_2370','CAE_10412.0','CAE_20110.0','CAE_23522.0','CAE_28140.0','CAE_26200.0',\n",
    "# 'CAE_23420.0','CAE_20594.0','Classe_2015','Classe_2030','CAE_23190.0','CAE_24450.0','CAE_28410.0','CAE_22210.0','CAE_31091.0',\n",
    "# 'CAE_23412.0','CAE_10320.0','CAE_32996.0','CAE_13203.0','CAE_23521.0','CAE_32130.0','CAE_27900.0','CAE_17290.0','Grupo_161',\n",
    "# 'CAE_19202.0','CAE_29100.0','CAE_28992.0','CAE_10411.0','CAE_20143.0','Grupo_104','CAE_32502.0','Classe_2712','Classe_1721',\n",
    "# 'CAE_27320.0','CAE_16102.0','CAE_14110.0','Classe_1621','Classe_3212','CAE_23200.0','CAE_20152.0','CAE_20411.0','Grupo_321',\n",
    "# 'CAE_25991.0','CAE_13950.0','CAE_15111.0','Grupo_107','Classe_2599','CAE_30920.0','CAE_11022.0','CAE_28960.0']\n",
    "\n",
    "#th=list(set(values_too_high))\n",
    "#df= df.drop(columns = th)#.dropna() # Dropna for Feature Selection purpose\n",
    "\n",
    "#Feature Engineering\n",
    "#flx=file[[\"NIPC_Ano\",\"exporter\",\"capitalIntensity\", \"VAB\", \"employess\", \"LaborProductivity\", \"Total_Vendas\", \"exporterIntensity\", \n",
    "#\"ProveitosGanhos\",\n",
    "#    \"TotalActivo\", \"MercadoInterno_PrestacoesServicos\", \"MercadoInterno_Fornecimentos\", \"Total_PrestacoesServicos\", \n",
    "#     \"Total_Fornecimentos\",\"CustosPessoal_Remuneracoes\", \"CustoMercadoriasMateriasConsumidas_Total\", \"ResultadoLiquidoExercicio\",\n",
    "#    \"TotalCapitalProprio\", \"Total_Compras\", \"CustoMercadoriasMateriasConsumidas_Materias\", \"CustosPessoal_Total\", \n",
    "#     \"CustoMercadoriasMateriasConsumidas_Mercadorias\", \"CustosPessoal_Outros\"]].dropna()\n",
    "\n",
    "df = df.rename(columns={\"VAB\":\"GVA\",\"VolumeNegocios\":\"Business Volume\", \"MercadoInterno_Fornecimentos\":\"Internal Market Supplies\", \"TotalActivo\":\"Total Assets\",\"Total_Compras\":\"Total Purchases\", \n",
    "                    \"CustosPessoal_Remuneracoes\":\"Personnel costs: Remuneration\", \"MercadoInterno_Compras\":\"Internal Market Purchases\", \"employess\":\"Employees\",\"CustosPessoal_Outros\":\"Other Personnel Costs\",\n",
    "                    \"TotalCapitalProprio\":\"Total Equity\",\"MercadoComunitario_Compras\":\"Community Market Purchases\", \"LaborProductivity\": \"Labor Productivity\",'MercadoComunitario_Fornecimentos':\"Community Market Supplies\",  \n",
    "                    'ResultadoLiquidoExercicio': \"Net Profit for the Year\",'MercadoInterno_Vendas':\"Internal Market Sales\", 'MercadoExtraComunitario_Compras':'Extra-Community Market Purchases',\n",
    "                    'MercadoExtraComunitario_Fornecimentos':\"Extra-Community Market Supplies\", 'capitalIntensity': \"Capital Intensity\",\n",
    "                    'MercadoExtraComunitario_PrestacaoesServicos':\"Extra-Community Market Services Rendered\", 'MercadoComunitario_PrestacoesServicos':\"Community Market Services Rendered\",\n",
    "                    'CustoMercadoriasMateriasConsumidas_Mercadorias':\"Cost of Material Goods Consumed\", \"exporter\":\"Exporter\"})\n",
    "\n",
    "df=df.set_index(\"NIPC_Ano\").dropna()# Dropna for Feature Selection purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de09b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funções de scoring\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def Score_Analytics (y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, np.round(y_pred,0)).ravel()\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    specificity = tn / (tn+fp)\n",
    "    Balanced_Accuracy = (sensitivity + specificity)/2\n",
    "    return sensitivity, specificity, Balanced_Accuracy\n",
    "\n",
    "def Exporter_Analytics(y_test,y_pred):\n",
    "    sensitivity, specificity, Balanced_Accuracy = Score_Analytics(y_test, y_pred)\n",
    "    try:\n",
    "        roc= roc_auc_score(y_test, y_pred)\n",
    "    except:\n",
    "        print(\"ROC not applicable to this case\")\n",
    "        roc = \"ROC doesn't exist\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    Num_Obs = len(y_test)\n",
    "\n",
    "    print(\"Balanced_Accuracy: \", Balanced_Accuracy)\n",
    "    print(\"Sensitivity: \", sensitivity)\n",
    "    print(\"Specificity: \", specificity)\n",
    "    print(\"ROC: \", roc)\n",
    "    print(\"Precision-Recall: \", pr_auc)\n",
    "    print(\"Num. Obs.: \",Num_Obs)\n",
    "    return sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs\n",
    "\n",
    "def adapter(tf, Export_group):\n",
    "    tf= tf.reset_index()\n",
    "    tf[\"NIPC\"]=tf['NIPC_Ano'].str[:9]\n",
    "    tf= tf[tf.NIPC.isin(Export_group)].set_index(\"NIPC_Ano\").drop(columns=\"NIPC\")\n",
    "    return tf\n",
    "\n",
    "def Exporter_Specific_Score(X_train,X_test,y_train,y_test, Export_group):\n",
    "    Xr = adapter(X_train,Export_group)    \n",
    "    Xs = adapter(X_test,Export_group)\n",
    "    NX_test=pd.concat([Xs,Xr])\n",
    "    Yr = adapter(y_train,Export_group)    \n",
    "    Ys = adapter(y_test,Export_group)\n",
    "    Ny_test=pd.concat([Ys,Yr])\n",
    "    return NX_test, Ny_test\n",
    "\n",
    "def Switch_Maker(flx):\n",
    "    flx[\"Ano\"]=flx['NIPC_Ano'].str[9:]\n",
    "    NIPCs_Swit_Export_t_1=[]\n",
    "    NIPCs_Swit_Export_t_2=[]\n",
    "    NIPCs_Swit_Export_t_3=[]\n",
    "    NIPCs_Swit_Export_t_4=[]\n",
    "    NIPCs_Swit_Export_t_5=[]\n",
    "    NIPCs_Swit_Export_t_6=[]\n",
    "    NIPCs_Swit_Export_t_7=[]\n",
    "    NIPCs_Swit_Export_t_8=[]\n",
    "    NIPCs_Swit_Export_t_9=[]\n",
    "    NIPCs_Swit_Export_t_10=[]\n",
    "    NIPCs_Swit_Export_t_11=[]\n",
    "    for nipc in NIPCs_Swit_Export:\n",
    "        list_exp=list(flx[flx['NIPC']==nipc].sort_values(by=\"Ano\").exporter)\n",
    "        if list_exp[1]==1:\n",
    "            NIPCs_Swit_Export_t_1.append(nipc)\n",
    "        elif list_exp[2]==1:\n",
    "            NIPCs_Swit_Export_t_2.append(nipc)\n",
    "        elif list_exp[3]==1:\n",
    "            NIPCs_Swit_Export_t_3.append(nipc)\n",
    "        elif list_exp[4]==1:\n",
    "            NIPCs_Swit_Export_t_4.append(nipc)\n",
    "        elif list_exp[5]==1:\n",
    "            NIPCs_Swit_Export_t_5.append(nipc)\n",
    "        elif list_exp[6]==1:\n",
    "            NIPCs_Swit_Export_t_6.append(nipc)\n",
    "        elif list_exp[7]==1:\n",
    "            NIPCs_Swit_Export_t_7.append(nipc)\n",
    "        elif list_exp[8]==1:\n",
    "            NIPCs_Swit_Export_t_8.append(nipc)\n",
    "        elif list_exp[9]==1:\n",
    "            NIPCs_Swit_Export_t_9.append(nipc)\n",
    "        elif list_exp[10]==1:\n",
    "            NIPCs_Swit_Export_t_10.append(nipc)\n",
    "        elif list_exp[11]==1:\n",
    "            NIPCs_Swit_Export_t_11.append(nipc)\n",
    "        else:\n",
    "            print(\"missing \",nipc)\n",
    "    flx = flx.drop(columns = \"Ano\")\n",
    "    return NIPCs_Swit_Export_t_1, NIPCs_Swit_Export_t_2, NIPCs_Swit_Export_t_3, NIPCs_Swit_Export_t_4, NIPCs_Swit_Export_t_5, NIPCs_Swit_Export_t_6, NIPCs_Swit_Export_t_7, NIPCs_Swit_Export_t_8, NIPCs_Swit_Export_t_9, NIPCs_Swit_Export_t_10, NIPCs_Swit_Export_t_11\n",
    "\n",
    "def Switch_Non_Exp_Maker(flx):\n",
    "    flx[\"Ano\"]=flx['NIPC_Ano'].str[9:]\n",
    "    NIPCs_Swit_Non_Export_t_1=[]\n",
    "    NIPCs_Swit_Non_Export_t_2=[]\n",
    "    NIPCs_Swit_Non_Export_t_3=[]\n",
    "    NIPCs_Swit_Non_Export_t_4=[]\n",
    "    NIPCs_Swit_Non_Export_t_5=[]\n",
    "    NIPCs_Swit_Non_Export_t_6=[]\n",
    "    NIPCs_Swit_Non_Export_t_7=[]\n",
    "    NIPCs_Swit_Non_Export_t_8=[]\n",
    "    NIPCs_Swit_Non_Export_t_9=[]\n",
    "    NIPCs_Swit_Non_Export_t_10=[]\n",
    "    NIPCs_Swit_Non_Export_t_11=[]\n",
    "    for nipc in NIPCs_Swit_NonExport:\n",
    "        list_exp=list(flx[flx['NIPC']==nipc].sort_values(by=\"Ano\").exporter)\n",
    "        if list_exp[1]==0:\n",
    "            NIPCs_Swit_Non_Export_t_1.append(nipc)\n",
    "        elif list_exp[2]==0:\n",
    "            NIPCs_Swit_Non_Export_t_2.append(nipc)\n",
    "        elif list_exp[3]==0:\n",
    "            NIPCs_Swit_Non_Export_t_3.append(nipc)\n",
    "        elif list_exp[4]==0:\n",
    "            NIPCs_Swit_Non_Export_t_4.append(nipc)\n",
    "        elif list_exp[5]==0:\n",
    "            NIPCs_Swit_Non_Export_t_5.append(nipc)\n",
    "        elif list_exp[6]==0:\n",
    "            NIPCs_Swit_Non_Export_t_6.append(nipc)\n",
    "        elif list_exp[7]==0:\n",
    "            NIPCs_Swit_Non_Export_t_7.append(nipc)\n",
    "        elif list_exp[8]==0:\n",
    "            NIPCs_Swit_Non_Export_t_8.append(nipc)\n",
    "        elif list_exp[9]==0:\n",
    "            NIPCs_Swit_Non_Export_t_9.append(nipc)\n",
    "        elif list_exp[10]==0:\n",
    "            NIPCs_Swit_Non_Export_t_10.append(nipc)\n",
    "        elif list_exp[11]==0:\n",
    "            NIPCs_Swit_Non_Export_t_11.append(nipc)\n",
    "        else:\n",
    "            print(\"missing \",nipc)\n",
    "    flx = flx.drop(columns = \"Ano\")\n",
    "    return NIPCs_Swit_Non_Export_t_1,NIPCs_Swit_Non_Export_t_2,NIPCs_Swit_Non_Export_t_3,NIPCs_Swit_Non_Export_t_4,NIPCs_Swit_Non_Export_t_5,NIPCs_Swit_Non_Export_t_6,NIPCs_Swit_Non_Export_t_7,NIPCs_Swit_Non_Export_t_8,NIPCs_Swit_Non_Export_t_9,NIPCs_Swit_Non_Export_t_10,NIPCs_Swit_Non_Export_t_11\n",
    "\n",
    "def Disc_Exp_Maker(flx):\n",
    "    flx[\"Ano\"]=flx['NIPC_Ano'].str[9:]\n",
    "    NIPCs_Disc_Export_1=[]\n",
    "    NIPCs_Disc_Export_2=[]\n",
    "    NIPCs_Disc_Export_3=[]\n",
    "    NIPCs_Disc_Export_4=[]\n",
    "    NIPCs_Disc_Export_5=[]\n",
    "    NIPCs_Disc_Export_6=[]\n",
    "    NIPCs_Disc_Export_7=[]\n",
    "    NIPCs_Disc_Export_8=[]\n",
    "    NIPCs_Disc_Export_9=[]\n",
    "    NIPCs_Disc_Export_10=[]\n",
    "    NIPCs_Disc_Export_11=[]\n",
    "    for nipc in NIPCs_Disc_Export:\n",
    "        exp_years = flx[flx[\"NIPC\"]==nipc].exporter.sum()\n",
    "        if exp_years==1:\n",
    "            NIPCs_Disc_Export_1.append(nipc)\n",
    "        elif exp_years==2:\n",
    "            NIPCs_Disc_Export_2.append(nipc)\n",
    "        elif exp_years==3:\n",
    "            NIPCs_Disc_Export_3.append(nipc)\n",
    "        elif exp_years==4:\n",
    "            NIPCs_Disc_Export_4.append(nipc)\n",
    "        elif exp_years==5:\n",
    "            NIPCs_Disc_Export_5.append(nipc)\n",
    "        elif exp_years==6:\n",
    "            NIPCs_Disc_Export_6.append(nipc)\n",
    "        elif exp_years==7:\n",
    "            NIPCs_Disc_Export_7.append(nipc)\n",
    "        elif exp_years==8:\n",
    "            NIPCs_Disc_Export_8.append(nipc)\n",
    "        elif exp_years==9:\n",
    "            NIPCs_Disc_Export_9.append(nipc)\n",
    "        elif exp_years==10:\n",
    "            NIPCs_Disc_Export_10.append(nipc)\n",
    "        elif exp_years==11:\n",
    "            NIPCs_Disc_Export_11.append(nipc)\n",
    "        else:\n",
    "            print(\"missing \",nipc)\n",
    "    flx = flx.drop(columns = \"Ano\")\n",
    "    return NIPCs_Disc_Export_1,NIPCs_Disc_Export_2,NIPCs_Disc_Export_3,NIPCs_Disc_Export_4,NIPCs_Disc_Export_5,NIPCs_Disc_Export_6,NIPCs_Disc_Export_7,NIPCs_Disc_Export_8,NIPCs_Disc_Export_9,NIPCs_Disc_Export_10,NIPCs_Disc_Export_11\n",
    "\n",
    "\n",
    "def Company_Size_NIPCs(flx):\n",
    "    flx[\"Ano\"]=flx['NIPC_Ano'].str[9:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection ##########################################################################################################\n",
    "#Feature Selection process - mutual_info_classif and SelectKBest\n",
    "X_test, X_train, y_test, y_train = preparing_train_test(df)\n",
    "\n",
    "##Trials\n",
    "#nr_feats_to_have = 150 #180 rep\n",
    "# resenta 0.9 da representatividade. 150 teve val_accuracy: 0.7914 c/70k obs\n",
    "\n",
    "relative_weight = True\n",
    "nr_weight = 0.9 #levou a 32 features\n",
    "\n",
    "final_features = Feature_Selection(X_train,y_train,relative_weight, nr_weight)\n",
    "pd.DataFrame(final_features).to_excel(\"V2/Features_selected.xlsx\")\n",
    "\n",
    "#Finding best features according to RFE\n",
    "#features, mask = RFE(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ea1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Control tests \n",
    "\n",
    "#df = df.fillna(df.median())\n",
    "\n",
    "#Getting values without company groups\n",
    "#df=df[~(df[\"large\"]==1)]\n",
    "#df= df.set_index(\"NIPC_Ano\")#.dropna()# Dropna for Feature Selection purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953deabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split #################################################################################\n",
    "#Final features\n",
    "feats_selected=pd.read_excel(\"V2/Features_selected.xlsx\")\n",
    "flx=df[list(feats_selected[0])+[\"Exporter\"]].dropna()\n",
    "#flx.to_excel(\"Last_frame.xlsx\")\n",
    "#Preparing final Train_test_split\n",
    "X_test, X_train, y_test, y_train = preparing_train_test(flx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6a861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flx.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85316219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "  \n",
    "flx =flx.drop(columns = [\"Business Volume\",\"Total Purchases\",\"Internal Market Purchases\",\n",
    "\"Community Market Purchases\",\"Extra-Community Market Purchases\"])\n",
    "# VIF dataframe \n",
    "vif_data = pd.DataFrame() \n",
    "vif_data[\"feature\"] = flx.columns \n",
    "  \n",
    "# calculating VIF for each feature \n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(flx.values, i) \n",
    "                          for i in range(len(flx.columns))] \n",
    "  \n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd91b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix ##############################################################################3333\n",
    "data = flx\n",
    "corr = data.corr()\n",
    "mask = (~(np.triu(np.ones_like(corr, dtype=bool))))\n",
    "plt.figure(figsize=(15, 12))\n",
    "ax = sns.heatmap(corr, mask = mask, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=200), square=True )\n",
    "ax.set_xticklabels( ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db37623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network Functions ######################################################################################\n",
    "class myCallback(keras.callbacks.Callback):\n",
    "        # Define the correct function signature for on_epoch_end\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if logs.get('accuracy') is not None and logs.get('accuracy') > 0.92:\n",
    "                print(\"\\nReached 92% accuracy so cancelling training!\")\n",
    "                # Stop training once the above condition is met\n",
    "                self.model.stop_training = True\n",
    "                \n",
    "def create_model_1(n_neurons,learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons, input_shape=(X_train.shape[1],), activation='relu')) # Add an input shape! (features,)\n",
    "    model.add(Dense(n_neurons, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary() \n",
    "    opt = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_model_2(n_neurons,n_neurons_2,learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons, input_shape=(X_train.shape[1],), activation='relu')) # Add an input shape! (features,)\n",
    "    model.add(Dense(n_neurons_2, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary() \n",
    "    opt = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_model_3(n_neurons,n_neurons_2,n_neurons_3,learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons, input_shape=(X_train.shape[1],), activation='relu')) # Add an input shape! (features,)\n",
    "    model.add(Dense(n_neurons_2, activation='relu'))\n",
    "    model.add(Dense(n_neurons_3, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary() \n",
    "    opt = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_model_3_adv(n_neurons, n_neurons_2, n_neurons_3, dropout, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons, input_shape=(X_train.shape[1],), activation='relu')) # Add an input shape! (features,)\n",
    "    model.add(Dropout(dropout))  # Adding a dropout layer with a dropout rate of 0.2\n",
    "    model.add(Dense(n_neurons_2, activation='relu'))\n",
    "    model.add(Dropout(dropout))  # Adding a dropout layer with a dropout rate of 0.2\n",
    "    model.add(Dense(n_neurons_3, activation='relu'))\n",
    "    model.add(Dropout(dropout))  # Adding a dropout layer with a dropout rate of 0.2\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary() \n",
    "    opt = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def sequential():\n",
    "    model = keras.Sequential()\n",
    "    model.add(Flatten(input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(units=128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=2, activation='softmax'))\n",
    "    model.summary() \n",
    "    opt = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def base_model1(learning_rate):\n",
    "    input_dim = X_train.shape[1]\n",
    "    model =Sequential([\n",
    "    Dense(32, input_dim = input_dim, activation= \"relu\"),\n",
    "    Dense(16, activation= \"relu\"),\n",
    "    Dense(1,activation = \"sigmoid\"),])\n",
    "    model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Nadam(learning_rate), metrics=[\"accuracy\"])\n",
    "    hist = model.fit(X_train,y_train, epochs = 100, validation_split=0.2, shuffle=True,verbose=1,batch_size=10,\n",
    "                     callbacks=[checkpoint,early,myCallback()])\n",
    "    return hist, model\n",
    "\n",
    "def adv_model1(n_neurons,learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons, input_shape = (X_train.shape[1],), activation= \"relu\"))\n",
    "    model.add(Dense(n_neurons, activation= \"relu\"))\n",
    "    model.add(Dense(1,activation = \"sigmoid\"))\n",
    "    model.summary() \n",
    "    opt = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=opt, metrics=[\"accuracy\"])\n",
    "    hist = model.fit(X_train,y_train, epochs = 100, validation_split=0.2, shuffle=True,verbose=1,batch_size=10,\n",
    "                     callbacks=[checkpoint,early,myCallback()])\n",
    "    return hist, model    \n",
    "\n",
    "def adv_modeld(n_neurons,n_neurons_2,learning_rate,dropout):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons, input_shape = (X_train.shape[1],), activation= \"relu\"))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_neurons_2, activation= \"relu\"))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1,activation = \"sigmoid\"))\n",
    "    model.summary() \n",
    "    opt = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=opt, metrics=[\"accuracy\"])\n",
    "    hist = model.fit(X_train,y_train, epochs = 100, validation_split=0.2, shuffle=True,verbose=1,batch_size=10,\n",
    "                     callbacks=[checkpoint,early,myCallback()])\n",
    "    return hist, model                \n",
    "    \n",
    "def adv_model2(n_neurons,learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons*2, input_shape = (X_train.shape[1],), activation= \"relu\"))\n",
    "    model.add(Dense(n_neurons, activation= \"relu\"))\n",
    "    model.add(Dense(n_neurons/2, activation= \"relu\"))\n",
    "    model.add(Dense(n_neurons, activation= \"relu\"))\n",
    "    model.add(Dense(n_neurons*2, activation= \"relu\"))\n",
    "    model.add(Dense(1,activation = \"sigmoid\"))\n",
    "    model.summary() \n",
    "    opt = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=opt, metrics=[\"accuracy\"])\n",
    "    hist = model.fit(X_train,y_train, epochs = 100, validation_split=0.2, shuffle=True,verbose=1,batch_size=10,\n",
    "                     callbacks=[checkpoint,early,myCallback()])\n",
    "    return hist, model \n",
    "\n",
    "checkpoint = [ModelCheckpoint(\"Exporters.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, \n",
    "                              save_weights_only=False, mode='max', save_freq='epoch')]\n",
    "\n",
    "early = EarlyStopping(monitor='val_accuracy', mode='max', patience=8, restore_best_weights=True)\n",
    "\n",
    "# Function for identifying scores for non-exporters in X_train and X_test\n",
    "def Non_Exporters_Score(flx,X_train,X_test):\n",
    "    Xr=X_train[X_train.index.isin(list(flx[flx[['NIPC_Ano',\"exporter\"]]['exporter']==0].NIPC_Ano.values))]\n",
    "    Xs=X_test[X_test.index.isin(list(flx[flx[['NIPC_Ano',\"exporter\"]]['exporter']==0].NIPC_Ano.values))]\n",
    "    Xn=pd.concat([Xs,Xr])\n",
    "    return Xneu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e96c49a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training the NN\n",
    "start_time = time.time()\n",
    "hist, model = adv_model2(n_neurons=64, learning_rate=0.0001)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c117e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting training\n",
    "history_dict = hist.history\n",
    "loss_values = history_dict['loss'] \n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1) \n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'orange', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6638c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = hist.history['accuracy']\n",
    "val_acc = hist.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'orange', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "np.max(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cb7194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791959b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline - X_test\n",
    "scores = model.predict(X_test)\n",
    "preds = np.round(scores,0)\n",
    "print(classification_report(y_test, preds))\n",
    "print(confusion_matrix(y_test, preds))\n",
    "sc= pd.DataFrame(scores,X_test.index) #Acrescentar index associado\n",
    "sc.to_csv(\"Scores_X_test.csv\")\n",
    "\n",
    "#Constant Exporters\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(y_test,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e2e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing for Micro, Small, Medium and Large companies \n",
    "for i in [\"NIPCs_Micro\",\"NIPCs_Small\",\"NIPCs_Medium\",\"NIPCs_Large\"]:   \n",
    "    print(i)\n",
    "    NX_test, Ny_test = Exporter_Specific_Score(X_train,X_test,y_train,y_test, eval(i))\n",
    "    Ny_pred = model.predict(NX_test)\n",
    "    sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(Ny_test,Ny_pred)\n",
    "    print(\"-----------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e81e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Scores for Non_Exporters\n",
    "Xn = Non_Exporters_Score(flx.reset_index(),X_train,X_test)\n",
    "scores = model.predict(Xn)\n",
    "#Exporting Scores to csv\n",
    "sx= pd.DataFrame(scores,Xn.index) #Acrescentar index associado\n",
    "sx.to_csv(\"Scores_Non_Exporters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b234df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table 4: Prediction accuracies and temporary trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c97478",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Categorizar NIPCs por perfi de empresa exportadora\n",
    "start_time = time.time()\n",
    "del(df)\n",
    "df=flx.reset_index()\n",
    "df[\"NIPC\"]=df['NIPC_Ano'].str[:9]\n",
    "df[\"Ano\"]=df['NIPC_Ano'].str[9:]\n",
    "NIPCs_Const_NonExport, NIPCs_Const_Export, NIPCs_Swit_Export, NIPCs_Swit_NonExport,NIPCs_Disc_Export, Non_Exp = Exporter_cat(df) \n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e62811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data integrity test, perfect\n",
    "#Ver os conjuntos de venn resultantes do código.\n",
    "#Que interseccoes existem entre os conjuntos ? Objetivo é ser 0.\n",
    "print(set(NIPCs_Const_Export).intersection(set(NIPCs_Const_NonExport)))\n",
    "print(set(NIPCs_Const_Export).intersection(set(NIPCs_Swit_Export)))\n",
    "print(set(NIPCs_Const_Export).intersection(set(NIPCs_Swit_NonExport)))\n",
    "print(set(NIPCs_Const_Export).intersection(set(NIPCs_Disc_Export)))\n",
    "print(set(NIPCs_Const_Export).intersection(set(Non_Exp)))\n",
    "print(set(NIPCs_Const_NonExport).intersection(set(NIPCs_Swit_Export)))\n",
    "print(set(NIPCs_Const_NonExport).intersection(set(NIPCs_Swit_NonExport)))\n",
    "print(set(NIPCs_Const_NonExport).intersection(set(NIPCs_Disc_Export)))\n",
    "print(set(NIPCs_Const_NonExport).intersection(set(Non_Exp)))\n",
    "print(set(NIPCs_Swit_Export).intersection(set(NIPCs_Swit_NonExport)))\n",
    "print(set(NIPCs_Swit_Export).intersection(set(NIPCs_Disc_Export)))\n",
    "print(set(NIPCs_Swit_Export).intersection(set(Non_Exp)))\n",
    "print(set(NIPCs_Swit_NonExport).intersection(set(NIPCs_Disc_Export)))\n",
    "print(set(NIPCs_Swit_NonExport).intersection(set(Non_Exp)))\n",
    "print(set(NIPCs_Disc_Export).intersection(set(Non_Exp)))\n",
    "print(len(df[df['NIPC'].isin(Non_Exp)]))\n",
    "print(\"Objetivo é ser tudo zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b3ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constant Exporters\n",
    "NX_test, Ny_test = Exporter_Specific_Score(X_train,X_test,y_train,y_test, NIPCs_Const_Export)\n",
    "Ny_pred = model.predict(NX_test)\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(Ny_test,Ny_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-Exporters\n",
    "NX_test, Ny_test = Exporter_Specific_Score(X_train,X_test,y_train,y_test, NIPCs_Const_NonExport)\n",
    "Ny_pred = model.predict(NX_test)\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(Ny_test,Ny_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898a4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switching to Export\n",
    "NX_test, Ny_test = Exporter_Specific_Score(X_train,X_test,y_train,y_test, NIPCs_Swit_Export)\n",
    "Ny_pred = model.predict(NX_test)\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(Ny_test,Ny_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switching to NonExport\n",
    "NX_test, Ny_test = Exporter_Specific_Score(X_train,X_test,y_train,y_test, NIPCs_Swit_NonExport)\n",
    "Ny_pred = model.predict(NX_test)\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(Ny_test,Ny_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d4251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discontinuous Export\n",
    "NX_test, Ny_test = Exporter_Specific_Score(X_train,X_test,y_train,y_test, NIPCs_Disc_Export)\n",
    "Ny_pred = model.predict(NX_test)\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(Ny_test,Ny_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c3278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switching to Export since tx\n",
    "NIPCs_Swit_Export_t_1, NIPCs_Swit_Export_t_2, NIPCs_Swit_Export_t_3, NIPCs_Swit_Export_t_4, NIPCs_Swit_Export_t_5, NIPCs_Swit_Export_t_6, NIPCs_Swit_Export_t_7, NIPCs_Swit_Export_t_8, NIPCs_Swit_Export_t_9, NIPCs_Swit_Export_t_10, NIPCs_Swit_Export_t_11 = Switch_Maker(flx[flx.NIPC.isin(NIPCs_Swit_Export)])\n",
    "for i in range(1,12):   \n",
    "    print(\"NIPCs_Swit_Export_t_\"+str(i))\n",
    "    NX_test, Ny_test = Exporter_Specific_Score(X_train,X_test,y_train,y_test, eval(\"NIPCs_Swit_Export_t_\"+str(i)))\n",
    "    Ny_pred = model.predict(NX_test)\n",
    "    sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(Ny_test,Ny_pred)\n",
    "    print(\"-----------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625f6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switching to Non_Export since tx\n",
    "NIPCs_Swit_Non_Export_t_1,NIPCs_Swit_Non_Export_t_2,NIPCs_Swit_Non_Export_t_3,NIPCs_Swit_Non_Export_t_4,NIPCs_Swit_Non_Export_t_5,NIPCs_Swit_Non_Export_t_6,NIPCs_Swit_Non_Export_t_7,NIPCs_Swit_Non_Export_t_8,NIPCs_Swit_Non_Export_t_9,NIPCs_Swit_Non_Export_t_10,NIPCs_Swit_Non_Export_t_11= Switch_Non_Exp_Maker(flx[flx.NIPC.isin(NIPCs_Swit_NonExport)])\n",
    "for i in range(1,12):   \n",
    "    print(\"NIPCs_Swit_Non_Export_t_\"+str(i))\n",
    "    NX_test, Ny_test = Exporter_Specific_Score(X_train,X_test,y_train,y_test, eval(\"NIPCs_Swit_Non_Export_t_\"+str(i)))\n",
    "    Ny_pred = model.predict(NX_test)\n",
    "    sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(Ny_test,Ny_pred)\n",
    "    print(\"-----------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893df74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Discontinuous Exporters by export experience\n",
    "NIPCs_Disc_Export_1,NIPCs_Disc_Export_2,NIPCs_Disc_Export_3,NIPCs_Disc_Export_4,NIPCs_Disc_Export_5,NIPCs_Disc_Export_6,NIPCs_Disc_Export_7,NIPCs_Disc_Export_8,NIPCs_Disc_Export_9,NIPCs_Disc_Export_10,NIPCs_Disc_Export_11 = Disc_Exp_Maker(flx[flx.NIPC.isin(NIPCs_Disc_Export)])\n",
    "for i in range(1,12):   \n",
    "    print(\"NIPCs_Disc_Export_\"+str(i))\n",
    "    NX_test, Ny_test = Exporter_Specific_Score(X_train,X_test,y_train,y_test, eval(\"NIPCs_Disc_Export_\"+str(i)))\n",
    "    Ny_pred = model.predict(NX_test)\n",
    "    sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(Ny_test,Ny_pred)\n",
    "    print(\"-----------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d9acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All sample\n",
    "Y = flx[['exporter']]#.set_index(\"NIPC_Ano\")\n",
    "X = flx.drop(['exporter'], axis=1)#.set_index(\"NIPC_Ano\")\n",
    "\n",
    "Y_pred = model.predict(X)\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(Y,Y_pred)\n",
    "\n",
    "preds_f = np.round(Y_pred,0)\n",
    "print(classification_report(Y, preds_f))\n",
    "print(confusion_matrix(Y, preds_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined a la Murakozy Permanent Exporters - Exporters with 4 consecutive years\n",
    "\n",
    "def count_consecutive_numbers(line):\n",
    "    ls_line=[int(line[i:i+4]) for i in range(0, len(line), 4)]\n",
    "    count = 0\n",
    "    max_count = 0\n",
    "    for i in range(len(ls_line) - 1):\n",
    "        if ls_line[i] + 1 == ls_line[i + 1]:\n",
    "            count += 1\n",
    "        else:\n",
    "            max_count = max(max_count, count)\n",
    "            count = 0\n",
    "    max_count = max(max_count, count)\n",
    "    return max_count + 1  # Adding 1 to include the last number in the sequence\n",
    "\n",
    "ff=flx.copy()\n",
    "ff=ff.reset_index()\n",
    "ff[\"NIPC\"] = ff.NIPC_Ano.str[:9]\n",
    "ff[\"Ano\"] = ff.NIPC_Ano.str[9:]\n",
    "fx=ff[ff[\"exporter\"]==1]\n",
    "xf = fx.groupby('NIPC')[['exporter',\"Ano\"]].sum()\n",
    "xf['consecutive_count'] = xf[\"Ano\"].apply(lambda row: count_consecutive_numbers(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c9af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select IDs where firms export at least 4 consecutive years\n",
    "NIPCs_Permanent_Exp = list(xf[xf[\"consecutive_count\"]>=4].index)\n",
    "NX_test, Ny_test = Exporter_Specific_Score(X_train,X_test,y_train,y_test, NIPCs_Permanent_Exp)\n",
    "Ny_pred = model.predict(NX_test)\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(Ny_test,Ny_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined a la Murakozy Temporary Exporters - Remaining Firms that export at least once\n",
    "NIPCs_Exp = set(fx.NIPC)\n",
    "NIPCs_Temp_Exp = set(NIPCs_Exp)-set(NIPCs_Permanent_Exp)\n",
    "NX_test, Ny_test = Exporter_Specific_Score(X_train,X_test,y_train,y_test, NIPCs_Temp_Exp)\n",
    "Ny_pred = model.predict(NX_test)\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(Ny_test,Ny_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66caa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff[ff[\"NIPC\"].isin(NIPCs_Temp_Exp)].to_excel(\"frame.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined a la Murakozy - Non-Exporters\n",
    "ff=flx.copy()\n",
    "ff=ff.reset_index()\n",
    "ff[\"NIPC\"] = ff.NIPC_Ano.str[:9]\n",
    "xo = ff.groupby('NIPC')['exporter'].sum()\n",
    "Non_Exp = list(xo[xo==0].index)\n",
    "\n",
    "NX_test, Ny_test = Exporter_Specific_Score(X_train,X_test,y_train,y_test, Non_Exp)\n",
    "Ny_pred = model.predict(NX_test)\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(Ny_test,Ny_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Exporter_Specific_Score_NX(X_train,X_test,y_train,y_test, test_group): \n",
    "    valid_X_train_group = [x for x in test_group if x in X_train.index]\n",
    "    Xr = X_train.loc[valid_X_train_group]\n",
    "    \n",
    "    valid_X_test_group = [x for x in test_group if x in X_test.index]\n",
    "    Xs = X_test.loc[valid_X_test_group]    \n",
    "    NX_test=pd.concat([Xs,Xr])\n",
    "    \n",
    "    valid_y_train_group = [x for x in test_group if x in y_train.index]\n",
    "    Yr = y_train.loc[valid_y_train_group]\n",
    "    \n",
    "    valid_y_test_group = [x for x in test_group if x in y_test.index]\n",
    "    Ys = y_test.loc[valid_y_test_group]    \n",
    "    Ny_test=pd.concat([Ys,Yr])\n",
    "\n",
    "    mask1 = ~X_train.index.isin(valid_X_train_group)\n",
    "    Xrt = X_train[mask1]\n",
    "\n",
    "    mask2 = ~X_test.index.isin(valid_X_test_group)\n",
    "    Xst = X_test[mask2]\n",
    "    \n",
    "    mask3 = ~y_train.index.isin(valid_y_train_group)\n",
    "    Yrt = y_train[mask3]\n",
    "    \n",
    "    mask4 = ~y_test.index.isin(valid_y_test_group)\n",
    "    Yst = y_test[mask4]\n",
    "    \n",
    "    NX_train=pd.concat([Xrt,Xst])\n",
    "    Ny_train=pd.concat([Yrt,Yst])\n",
    "\n",
    "    return NX_test, Ny_test, NX_train, Ny_train\n",
    "\n",
    "def adv_model2_pset(X_train, y_train, n_neurons,learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons*2, input_shape = (X_train.shape[1],), activation= \"relu\"))\n",
    "    model.add(Dense(n_neurons, activation= \"relu\"))\n",
    "    model.add(Dense(n_neurons/2, activation= \"relu\"))\n",
    "    model.add(Dense(n_neurons, activation= \"relu\"))\n",
    "    model.add(Dense(n_neurons*2, activation= \"relu\"))\n",
    "    model.add(Dense(1,activation = \"sigmoid\"))\n",
    "    model.summary() \n",
    "    opt = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=opt, metrics=[\"accuracy\"])\n",
    "    hist = model.fit(X_train,y_train, epochs = 100, validation_split=0.2, shuffle=True,verbose=1,batch_size=10,\n",
    "                     callbacks=[checkpoint,early,myCallback()])\n",
    "    return hist, model\n",
    "\n",
    "checkpoint = [ModelCheckpoint(\"Exporters.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, \n",
    "                              save_weights_only=False, mode='max', save_freq='epoch')]\n",
    "\n",
    "early = EarlyStopping(monitor='val_accuracy', mode='max', patience=8, restore_best_weights=True)\n",
    "\n",
    "def preparing_train_test_post(tflx, exp, features):\n",
    "\n",
    "    # split into X and Y\n",
    "\n",
    "    flx=tflx[list(features[0])+[exp]]\n",
    "    Y = flx[[exp]]\n",
    "    X = flx.drop([exp], axis=1)\n",
    "\n",
    "    cols = X.columns  \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    X=pd.DataFrame(X, columns = cols, index = flx.index).reset_index()\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "\n",
    "    #Original\n",
    "    X_test = X[X.NIPC_Ano.str[:9].str.contains('|'.join(list(set(nipcs.value.astype(str)))))] \n",
    "    X_train = X[~(X.NIPC_Ano.str[:9].str.contains('|'.join(list(set(nipcs.value.astype(str))))))] \n",
    "    y_test = Y[Y.index.str[:9].str.contains('|'.join(list(set(nipcs.value.astype(str)))))]\n",
    "    y_train = Y[~(Y.index.str[:9].str.contains('|'.join(list(set(nipcs.value.astype(str))))))]\n",
    "        \n",
    "    X_train = X_train.set_index(\"NIPC_Ano\")#.drop(['NIPC_Ano'], axis=1)\n",
    "    X_test = X_test.set_index(\"NIPC_Ano\")#.drop(['NIPC_Ano'], axis=1)\n",
    "    #y_train = y_train.set_index(\"NIPC_Ano\")#.drop(['NIPC_Ano'], axis=1)\n",
    "    #y_test = y_test.set_index(\"NIPC_Ano\")#.drop(['NIPC_Ano'], axis=1)\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    return X_test, X_train, y_test, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Test Split for alternative definitions of exporters #################################################################################\n",
    "#Final features\n",
    "feats_selected=pd.read_excel(\"V2/Features_selected.xlsx\")\n",
    "flx=df[list(feats_selected[0])+[\"Exporter\"]].dropna()\n",
    "\n",
    "#Creating features for future scoring\n",
    "filex = file.copy()\n",
    "filex[\"NIPC_Ano\"] = filex[\"NIPC\"].astype(str) + filex[\"Ano\"].astype(str)\n",
    "\n",
    "#Scenario 1)\n",
    "filex[\"MercadoExterno_Vendas_Racio\"] = (filex[\"Total_Vendas\"] - filex[\"MercadoInterno_Vendas\"])/filex[\"Total_Vendas\"]\n",
    "filex[\"Export_at_least_5_p_cent_abroad\"] = np.where(filex[\"MercadoExterno_Vendas_Racio\"]>0.05,1,0 )\n",
    "\n",
    "#Scenario 2)\n",
    "filex[\"Export_at_least_15_p_cent_abroad\"] = np.where(filex[\"MercadoExterno_Vendas_Racio\"]>0.15,1,0 )\n",
    "\n",
    "#Scenario 3)\n",
    "filex[\"Export_Outside_EU\"] = np.where(filex[\"MercadoExtraComunitario_Vendas\"]>0,1,0 )\n",
    "\n",
    "#Scenario 4)\n",
    "destinations [\"NIPC_Ano\"] = destinations[\"_NIF\"].astype(str)+destinations[\"Ano\"].astype(str)\n",
    "destinations[\"More_than_two_markets\"] = np.where(destinations[\"Paises\"]>2,1,0 )\n",
    "\n",
    "#Scenario 5)\n",
    "destinations[\"More_than_five_markets\"] = np.where(destinations[\"Paises\"]>5,1,0 )\n",
    "\n",
    "#Scenario 6)\n",
    "destinations[\"More_than_nine_markets\"] = np.where(destinations[\"Paises\"]>9,1,0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc56610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Predictions with NN for different definitions of exporter - Firms that sell at least 5% of total sales abroad\n",
    "xf=pd.merge(filex[[\"NIPC_Ano\",\"Export_at_least_5_p_cent_abroad\"]], flx, on=\"NIPC_Ano\")\n",
    "xf= xf.set_index(\"NIPC_Ano\")\n",
    "ff=xf.drop(columns=\"Exporter\")\n",
    "ff=ff.dropna()\n",
    "X_test, X_train, y_test, y_train = preparing_train_test_post(ff, \"Export_at_least_5_p_cent_abroad\", feats_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799547bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the NN\n",
    "start_time = time.time()\n",
    "hist, model = adv_model2_pset(X_train,y_train,n_neurons=64, learning_rate=0.0001)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe036c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Predictions with NN for different definitions of exporter - Firms that sell at least 15% of total sales abroad\n",
    "\n",
    "xf=pd.merge(filex[[\"NIPC_Ano\",\"Export_at_least_15_p_cent_abroad\"]], flx, on=\"NIPC_Ano\")\n",
    "xf= xf.set_index(\"NIPC_Ano\")\n",
    "ff=xf.drop(columns=\"Exporter\")\n",
    "ff=ff.dropna()\n",
    "X_test, X_train, y_test, y_train = preparing_train_test_post(ff, \"Export_at_least_15_p_cent_abroad\", feats_selected)\n",
    "\n",
    "#Training the NN\n",
    "start_time = time.time()\n",
    "hist, model = adv_model2_pset(X_train,y_train,n_neurons=64, learning_rate=0.0001)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "y_pred = model.predict(X_test)\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b0573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) Predictions with NN for different definitions of exporter - Exporting outside the EU market\n",
    "\n",
    "xf=pd.merge(filex[[\"NIPC_Ano\",\"Export_Outside_EU\"]], flx, on=\"NIPC_Ano\")\n",
    "xf= xf.set_index(\"NIPC_Ano\")\n",
    "ff=xf.drop(columns=\"Exporter\")\n",
    "ff=ff.dropna()\n",
    "X_test, X_train, y_test, y_train = preparing_train_test_post(ff, \"Export_Outside_EU\", feats_selected)\n",
    "\n",
    "#Training the NN\n",
    "start_time = time.time()\n",
    "hist, model = adv_model2_pset(X_train,y_train,n_neurons=64, learning_rate=0.0001)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "y_pred = model.predict(X_test)\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d5457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) Predictions with NN for different definitions of exporter - Firms that sell to at least 3 countries\n",
    "\n",
    "xf=pd.merge(destinations[[\"NIPC_Ano\",\"More_than_two_markets\"]], flx, on=\"NIPC_Ano\")\n",
    "xf= xf.set_index(\"NIPC_Ano\")\n",
    "ff=xf.drop(columns=\"Exporter\")\n",
    "ff=ff.dropna()\n",
    "X_test, X_train, y_test, y_train = preparing_train_test_post(ff, \"More_than_two_markets\", feats_selected)\n",
    "\n",
    "#Training the NN\n",
    "start_time = time.time()\n",
    "hist, model = adv_model2_pset(X_train,y_train,n_neurons=64, learning_rate=0.0001)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "y_pred = model.predict(X_test)\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) Predictions with NN for different definitions of exporter - Firms that sell to at least 6 countries\n",
    "\n",
    "xf=pd.merge(destinations[[\"NIPC_Ano\",\"More_than_five_markets\"]], flx, on=\"NIPC_Ano\")\n",
    "xf= xf.set_index(\"NIPC_Ano\")\n",
    "ff=xf.drop(columns=\"Exporter\")\n",
    "ff=ff.dropna()\n",
    "X_test, X_train, y_test, y_train = preparing_train_test_post(ff, \"More_than_five_markets\", feats_selected)\n",
    "\n",
    "#Training the NN\n",
    "start_time = time.time()\n",
    "hist, model = adv_model2_pset(X_train,y_train,n_neurons=64, learning_rate=0.0001)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "y_pred = model.predict(X_test)\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc72b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6) Predictions with NN for different definitions of exporter - Firms that sell to at least 10 countries\n",
    "\n",
    "xf=pd.merge(destinations[[\"NIPC_Ano\",\"More_than_nine_markets\"]], flx, on=\"NIPC_Ano\")\n",
    "xf= xf.set_index(\"NIPC_Ano\")\n",
    "ff=xf.drop(columns=\"Exporter\")\n",
    "ff=ff.dropna()\n",
    "X_test, X_train, y_test, y_train = preparing_train_test_post(ff, \"More_than_nine_markets\", feats_selected)\n",
    "\n",
    "#Training the NN\n",
    "start_time = time.time()\n",
    "hist, model = adv_model2_pset(X_train,y_train,n_neurons=64, learning_rate=0.0001)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "y_pred = model.predict(X_test)\n",
    "sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b660867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test by year - first take. (In here we were training on the original sample)\n",
    "ff=flx.copy()\n",
    "ff=ff.reset_index()\n",
    "ff[\"Ano\"] = ff.NIPC_Ano.str[9:]\n",
    "\n",
    "def Exporter_Specific_Score_Y(X_train,X_test,y_train,y_test, test_group):\n",
    "    \n",
    "    valid_X_train_group = [x for x in test_group if x in X_train.index]\n",
    "    Xr = X_train.loc[valid_X_train_group]\n",
    "    valid_X_test_group = [x for x in test_group if x in X_test.index]\n",
    "    Xs = X_test.loc[valid_X_test_group]    \n",
    "    NX_test=pd.concat([Xs,Xr])\n",
    "    valid_y_train_group = [x for x in test_group if x in y_train.index]\n",
    "    Yr = y_train.loc[valid_y_train_group]\n",
    "    valid_y_test_group = [x for x in test_group if x in y_test.index]\n",
    "    Ys = y_test.loc[valid_y_test_group]    \n",
    "    Ny_test=pd.concat([Ys,Yr])\n",
    "    return NX_test, Ny_test\n",
    "\n",
    "for year in set(ff.Ano):\n",
    "    print(year)\n",
    "    test_group = list(ff[ff[\"Ano\"] == str(year)].NIPC_Ano)\n",
    "    NX_test, Ny_test = Exporter_Specific_Score_Y(X_train,X_test,y_train,y_test, test_group)\n",
    "    Ny_pred = model.predict(NX_test)\n",
    "    sensitivity, specificity, Balanced_Accuracy, roc, pr_auc, Num_Obs = Exporter_Analytics(Ny_test,Ny_pred)\n",
    "    print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test by year - second take. In a different way - Now we will change what we train to train on all other years\n",
    "ff=flx.copy()\n",
    "ff=ff.reset_index()\n",
    "ff[\"Ano\"] = ff.NIPC_Ano.str[9:]\n",
    "\n",
    "my_dict = {}\n",
    "\n",
    "for year in set(ff.Ano):\n",
    "    print(year)\n",
    "\n",
    "    my_dict[\"year_\"+year] = year\n",
    "    my_dict[\"test_group_\"+year] = list(ff[ff[\"Ano\"] == str(year)].NIPC_Ano)\n",
    "    my_dict[\"X_test_\"+year], my_dict[\"y_test_\"+year],my_dict[\"X_train_\"+year], my_dict[\"y_train_\"+year] = Exporter_Specific_Score_NX(X_train,X_test,y_train,y_test, my_dict[\"test_group_\"+year])\n",
    "    #Training the NN\n",
    "    start_time = time.time()\n",
    "    my_dict[\"hist_\"+year], my_dict[\"model_\"+year] = adv_model2_pset(my_dict[\"X_train_\"+year],my_dict[\"y_train_\"+year],n_neurons=64, learning_rate=0.0001)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
    "    print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "    my_dict[\"y_pred_\"+year] = my_dict[\"model_\"+year].predict(my_dict[\"X_test_\"+year])\n",
    "    my_dict[\"sensitivity_\"+year], my_dict[\"specificity_\"+year], my_dict[\"Balanced_Accuracy_\"+year], my_dict[\"ROC_\"+year], my_dict[\"PR_AUC_\"+year], my_dict[\"Numb_Obs_\"+year] = Exporter_Analytics(my_dict[\"y_test_\"+year],my_dict[\"y_pred_\"+year])\n",
    "    print(\"---------------------------------------------------------\")\n",
    "\n",
    "#    year_data = {\"year\": year, \"test_group\": test_group, \"NX_test\": NX_test, \"Ny_test\": Ny_test, \"NX_train\": NX_train, \"Ny_train\": Ny_train, \"hist\": hist, \"model\": model,\n",
    "#    \"elapsed_time\": elapsed_time, \"sensitivity\": sensitivity, \"specificity\": specificity,  \"Balanced_Accuracy\": Balanced_Accuracy, \"roc\": roc, \"pr_auc\": pr_auc, \"Num_Obs\": Num_Obs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb70a9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plotting Scores\n",
    "sns.displot(Y_pred, kind=\"kde\", bw_adjust=.2) #Distribution plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "print(confusion_matrix(y_test['exporter'], preds))\n",
    "#print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21483f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP values for interpretability\n",
    "#explainer = shap.Explainer(model)\n",
    "explainer = shap.Explainer(model.predict, X_test)\n",
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afbd0c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)\n",
    "shap.summary_plot(shap_values)\n",
    "shap.plots.beeswarm(shap_values)\n",
    "shap.plots.bar(shap_values[0])\n",
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea512300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
